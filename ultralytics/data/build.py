# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import os
import random
from pathlib import Path

import numpy as np
import torch
from PIL import Image
import torch.utils.data as data
from torch.utils.data import dataloader, distributed
from ultralytics.utils import LOGGER
from ultralytics.data.dataset import GroundingDataset, YOLODataset, YOLOVideoDataset, YOLOMultiModalDataset, YOLOStreamDataset
from ultralytics.data.loaders import (
    LOADERS,
    LoadImagesAndVideos,
    LoadPilAndNumpy,
    LoadScreenshots,
    LoadStreams,
    LoadTensor,
    LoadTensorBuffer,
    SourceTypes,
    autocast_list,
)
from ultralytics.data.utils import IMG_FORMATS, PIN_MEMORY, VID_FORMATS
from ultralytics.utils import RANK, colorstr
from ultralytics.utils.checks import check_file

import torch
import random
from torch.utils.data import Sampler
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist

class StreamSampler(DistributedSampler):
    '''
    Streaming Sampling Datasets from Video
    '''
    def __init__(self, dataset, batch_size, seed=10,distributed=False):
        self.dataset = dataset
        self.batch_size = batch_size

        self.seed = seed
        if distributed:
            self.rank = dist.get_rank()  # Get the rank of the current process
            self.world_size = dist.get_world_size()  # Get the total number of processes
        else:
            self.rank = 0
            self.world_size = 1
            
        self.last_next_frame_index = [None for i in range(int(batch_size))]
        self.next_train_video_index = 0

        # random.seed(self.seed)
        # indices = list(range(len(self.dataset.sub_video_splits)))
        # random.shuffle(indices)
        
        # indices_per_rank = len(indices) // self.world_size  #  Data subset size per process
        # start_idx = self.rank * indices_per_rank
        # end_idx = (self.rank + 1) * indices_per_rank
        
        # self.indices = indices[start_idx:end_idx]  # Video subset of the current process
        # self.indices = self.dataset.muti_rank_indices_splits[self.rank]
        self.data_length = self.dataset.per_gpu_total_frames - self.dataset.per_gpu_total_frames%self.batch_size

        print(f"worksize:{self.world_size}, rank: {self.rank}")
    def __iter__(self):

        self.last_next_frame_index = [None for i in range(int(self.batch_size))]
        self.next_train_video_index = 0
        self.indices = self.dataset.muti_rank_indices_splits[self.rank]
        
        random.seed(44)
        random.shuffle(self.indices)
        
        self.data_length = self.dataset.per_gpu_total_frames - self.dataset.per_gpu_total_frames%self.batch_size
        # print(f"sampler init: {len(self.indices)}")

        i = 0
        for _ in range(self.data_length):
            if i == self.batch_size:
                i = 0
            
            if self.last_next_frame_index[i] is not None:
                ix,iy = self.last_next_frame_index[i] # Video indexing and frame indexing
                index = self.dataset.get_index_from_sub(ix,iy)
                cur_sampler_is_train = self.dataset.img_video_info[index]["is_train"]
            else: 
                cur_sampler_is_train = False
                
            if cur_sampler_is_train:                        #Current Video Continue Training
                yield index
                if iy + 1 == len(self.dataset.sub_video_splits[ix]):
                    self.last_next_frame_index[i] = None    #Video training complete.
                else:
                    self.last_next_frame_index[i][-1] += 1  #Continue training this video using the next frame
            else:                                           #Switching Video Training
                ix, iy = self.indices[self.next_train_video_index], -1
                self.next_train_video_index += 1
                if self.next_train_video_index == len(self.indices):
                    self.next_train_video_index = 0

                is_train = False
                # if not train, next
                while(not is_train):
                    iy += 1
                    index = self.dataset.get_index_from_sub(ix, iy)
                    is_train = self.dataset.img_video_info[index]["is_train"] if self.dataset.augment else True
                    if iy + 1 == len(self.dataset.sub_video_splits[ix]): 
                        self.last_next_frame_index[i] = None #Video training complete.
                    else:
                        self.last_next_frame_index[i] = [ix, iy+1] #Video Continues Training
                yield index
                
            i += 1
            
        self.last_next_frame_index = [None for i in self.last_next_frame_index]
        self.next_train_video_ix = 0
        
    def _epoch_reset(self):
        self.last_next_frame_index = [None for i in self.last_next_frame_index]
        self.next_train_video_ix = 0 

    def __len__(self):
        return self.data_length


class VideoSampler(DistributedSampler):
    '''
    Streaming Sampling Datasets from Video
    '''
    def __init__(self, dataset, batch_size, seed=10, distributed=False, shuffle=True):
        # Check if distributed mode is enabled and process group is already initialized
        self.dataset = dataset  # self.sub_videos
        self.batch_size = batch_size
        self.seed = seed
        self.epoch = 0
        self.distributed = distributed
        self.shuffle = shuffle
        
        if distributed:
            self.rank = dist.get_rank()  # Get the rank of the current process
            self.world_size = dist.get_world_size()  # Get the total number of processes
        else:
            self.rank = 0
            self.world_size = 1
        
    def __iter__(self):
        # Generate indices for sub-videos (each sub-video is a list of frames)
        indices = list(self.dataset.sampler_indices[self.rank])
        
        # Shuffle if needed (for training)
        if self.shuffle:
            # deterministically shuffle based on epoch and seed
            rng = random.Random(self.seed + self.epoch)
            rng.shuffle(indices)
            if self.rank == 0 or self.rank == -1:
                print(f"epoch: {self.epoch}, shuffle indices: {indices[:5]}")
            
        for i in indices:
            yield i

    def set_epoch(self, epoch):
        self.epoch = epoch

    def __len__(self):
        return len(self.dataset.sampler_indices[self.rank])  # Number of batches

    def _epoch_reset(self):
        '''
        Reset for a new epoch: re-calculate the video frame indices and re-shuffle the dataset.
        This is necessary when the sub-video divisions change.
        '''
        # Recalculate the video frame indices if necessary (if you modified all_sub_videos)
        self.video_frame_indices = self.dataset._get_video_frame_indices()

        # Reset any other internal state as needed
        self.seed += 1  # Optionally increment the seed for the next epoch
        self.epoch += 1  # Increment the epoch
        self.set_epoch(self.epoch)  # Update the epoch in the distributed sampler

from typing import Iterator, List, Optional, Dict, Any
class StreamDistributedSampler(DistributedSampler):
    """
    åˆ†å¸ƒå¼æˆ–å•å¡è§†é¢‘æµé‡‡æ ·å™¨ï¼Œé¢„å…ˆç”Ÿæˆæ¯ä¸ª epoch çš„å¸§ç´¢å¼•åˆ—è¡¨ã€‚

    ç»§æ‰¿è‡ª DistributedSampler (ç”¨äºå¤„ç†åˆ†å¸ƒå¼é€»è¾‘) æˆ–ä½œä¸ºåŸºç¡€ Sampler (å•å¡æ¨¡å¼)ã€‚
    æ ¹æ® `distribute` å‚æ•°å’Œ `batch_size`ã€`world_size` (è®­ç»ƒæ¨¡å¼ä¸‹) è°ƒæ•´å­è§†é¢‘æ•°é‡ï¼Œ
    å¹¶å°†åˆ†é…åˆ°çš„å­è§†é¢‘çš„å¸§ç´¢å¼•æŒ‰æŒ‡å®šæ ¼å¼é«˜æ•ˆå±•å¹³å­˜å‚¨ã€‚

    Args:
        dataset (YOLOStreamDataset): æ•°æ®é›†å¯¹è±¡ï¼Œå¿…é¡»åŒ…å« sub_videos (List[List[int]])
                                     å’Œ is_training_augment (bool) å±æ€§ã€‚
        batch_size (int): DataLoader ä½¿ç”¨çš„ batch size (æ¯æ‰¹æ¬¡çš„æ€»å¸§æ•°)ã€‚
                          åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¿™ç”¨äºç¡®å®šè¦ä½¿ç”¨çš„å­è§†é¢‘æ€»æ•°åº”æ˜¯å…¶å€æ•°ã€‚
                          æ³¨æ„ï¼šè¿™é‡Œ batch_size çš„ä½¿ç”¨æ–¹å¼ä¸ DataLoader ç›´æ¥æ‰¹å¤„ç†å¸§ä¸åŒï¼Œ
                          å®ƒå½±å“çš„æ˜¯å‚ä¸é‡‡æ ·çš„å­è§†é¢‘æ€»æ•°ã€‚
        distribute (bool, optional): æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·ã€‚å¦‚æœä¸º Falseï¼Œåˆ™åœ¨å•å¡æ¨¡å¼ä¸‹è¿è¡Œï¼Œ
                                     å¿½ç•¥ num_replicas å’Œ rank å‚æ•° (æˆ–ä½¿ç”¨é»˜è®¤å€¼ 1 å’Œ 0)ã€‚
                                     é»˜è®¤ä¸º Trueã€‚
        num_replicas (int, optional): åˆ†å¸ƒå¼è®­ç»ƒçš„è¿›ç¨‹/å‰¯æœ¬æ€»æ•°ã€‚é»˜è®¤ä» torch.distributed è·å– (å¦‚æœ distribute=True)ã€‚
        rank (int, optional): å½“å‰è¿›ç¨‹/å‰¯æœ¬çš„æ’åã€‚é»˜è®¤ä» torch.distributed è·å– (å¦‚æœ distribute=True)ã€‚
        shuffle (bool, optional): æ˜¯å¦åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶æ‰“ä¹±å­è§†é¢‘é¡ºåºã€‚é»˜è®¤ä¸º Trueã€‚
        seed (int, optional): éšæœºç§å­ï¼Œç”¨äºæ‰“ä¹±å­è§†é¢‘é¡ºåºã€‚é»˜è®¤ä¸º 0ã€‚
        drop_last (bool): æ³¨æ„: æ­¤å‚æ•°ç»§æ‰¿è‡ª DistributedSampler ä½†ä¸ç›´æ¥æ§åˆ¶æœ€ç»ˆå¸§æ‰¹æ¬¡çš„ä¸¢å¼ƒã€‚
                          æˆ‘ä»¬æ ¹æ® batch_size å’Œ world_size åœ¨åˆå§‹åŒ–æ—¶è¿›è¡Œå­è§†é¢‘æ•°é‡çš„è°ƒæ•´ã€‚
                          çˆ¶ç±»çš„ drop_last ä¸ä¼šå½±å“æœ¬é‡‡æ ·å™¨æœ€ç»ˆ yielded çš„å¸§æ•°é‡ã€‚
                          ä¿ç•™æ­¤å‚æ•°ä¸»è¦æ˜¯ä¸ºäº†å…¼å®¹çˆ¶ç±»ç­¾åã€‚
    """
    def __init__(self,
                 dataset, # ç±»å‹æç¤ºåº”ä¸º Datasetï¼Œä½†æˆ‘ä»¬æœŸæœ› YOLOStreamDataset
                 batch_size: int,
                 distributed: bool = True, # æ–°å¢å‚æ•°ï¼šæ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼é‡‡æ ·
                 num_replicas: Optional[int] = None,
                 rank: Optional[int] = None,
                 shuffle: bool = True,
                 seed: int = 0,
                 drop_last: bool = False): # drop_last å‚æ•°ä¿ç•™ç”¨äºç­¾åå…¼å®¹æ€§

        self.distribute = distributed # å­˜å‚¨åˆ†å¸ƒå¼æ¨¡å¼æ ‡å¿—

        # æ ¹æ® distribute æ ‡å¿—ç¡®å®šå®é™…ç”¨äºé‡‡æ ·çš„ num_replicas å’Œ rank
        if self.distribute:
            # å¦‚æœæ˜¾å¼æä¾›äº† num_replicas å’Œ rankï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
            # å¦åˆ™ï¼Œä» torch.distributed è·å–
            if num_replicas is None:
                if not dist.is_initialized():
                    raise RuntimeError("åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ (distribute=True)ï¼Œtorch.distributed å¿…é¡»å·²åˆå§‹åŒ–æˆ–å¿…é¡»æ˜¾å¼æä¾› num_replicas å’Œ rank å‚æ•°ã€‚")
                num_replicas = dist.get_world_size()
            if rank is None:
                if not dist.is_initialized():
                    raise RuntimeError("åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ (distribute=True)ï¼Œtorch.distributed å¿…é¡»å·²åˆå§‹åŒ–æˆ–å¿…é¡»æ˜¾å¼æä¾› num_replicas å’Œ rank å‚æ•°ã€‚")
                rank = dist.get_rank()
            # åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œç¡®ä¿ num_replicas å’Œ rank æ˜¯é None çš„
            assert num_replicas is not None and rank is not None

        else: # å•å¡æ¨¡å¼
            # åœ¨å•å¡æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶ num_replicas=1, rank=0
            num_replicas = 1
            rank = 0
            if dist.is_initialized():
                 # å¦‚æœ dist å·²åˆå§‹åŒ–ä½†æŒ‡å®šäº†å•å¡æ¨¡å¼ï¼Œå‘å‡ºè­¦å‘Š
                 LOGGER.warning("StreamDistributedSampler æŒ‡å®š distribute=Falseï¼Œä½†åœ¨å·²åˆå§‹åŒ–çš„åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œã€‚å°†ä½¿ç”¨å•å¡é‡‡æ ·é€»è¾‘ï¼Œå¿½ç•¥è¿›ç¨‹ç»„ä¿¡æ¯ã€‚")


        # è°ƒç”¨çˆ¶ç±» __init__ã€‚æ­¤æ—¶ï¼Œå®é™…çš„ num_replicas å’Œ rank å·²ç»è¢«ç¡®å®šå¹¶ä¼ é€’ç»™çˆ¶ç±»ã€‚
        # çˆ¶ç±»å°†ä½¿ç”¨è¿™äº›å€¼æ¥è®¾ç½®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå°½ç®¡åœ¨å•å¡æ¨¡å¼ä¸‹è¿™äº›å€¼æ˜¯ 1 å’Œ 0ã€‚
        # æˆ‘ä»¬ä¸»è¦ç»§æ‰¿çˆ¶ç±»æ¥è·å–å…¶å±æ€§è®¾ç½®å’Œ set_epoch çš„åŸºæœ¬é€»è¾‘ã€‚
        super().__init__(dataset, num_replicas, rank, shuffle, seed, drop_last)

        if batch_size <= 0:
            raise ValueError(f"batch_size å¿…é¡»æ˜¯æ­£æ•°ï¼Œä½†å¾—åˆ° {batch_size}")

        # éªŒè¯æ•°æ®é›†æ˜¯å¦åŒ…å«å¿…è¦çš„å±æ€§
        if not hasattr(dataset, 'sub_videos') or not isinstance(dataset.sub_videos, list) or \
           not hasattr(dataset, 'is_training_augment'):
             raise TypeError("æ•°æ®é›†å¿…é¡»æ˜¯ YOLOStreamDataset å®ä¾‹æˆ–æ‹¥æœ‰ 'sub_videos' (åˆ—è¡¨çš„åˆ—è¡¨) å’Œ 'is_training_augment' (å¸ƒå°”å€¼) å±æ€§ã€‚")

        self.dataset = dataset # å­˜å‚¨æ•°æ®é›†å¯¹è±¡
        self.batch_size = batch_size
        self.is_training = dataset.augment # ä»æ•°æ®é›†è·å–è®­ç»ƒæ¨¡å¼

        self._num_original_sub_videos = len(dataset.sub_videos) # è°ƒæ•´/é€‰æ‹©å‰çš„å­è§†é¢‘æ€»æ•°
        self.flat_frame_indices: List[int] = [] # è¿™å°†å­˜å‚¨å½“å‰å‘¨æœŸçš„å±•å¹³å¸§ç´¢å¼•åˆ—è¡¨

        # mode_str = "åˆ†å¸ƒå¼æ¨¡å¼" if self.distribute else "å•å¡æ¨¡å¼"
        # LOGGER.info(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: åˆå§‹åŒ– StreamDistributedSampler ({mode_str}ï¼Œå‘¨æœŸ 0)ã€‚"
        #             f"è®­ç»ƒæ¨¡å¼: {self.is_training}ã€‚æ‰¹æ¬¡å¤§å° (ç”¨äºä¿®å‰ª): {self.batch_size}ã€‚")
        # LOGGER.info(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: å‘¨æœŸ 0 çš„æ€»å¸§ç´¢å¼•æ•°: {len(self.flat_frame_indices)}")
        self._recalculate_indices()
        # è®°å½•é‡æ–°è®¡ç®—çš„ç»“æœ
        mode_str = "åˆ†å¸ƒå¼æ¨¡å¼" if self.distribute else "å•å¡æ¨¡å¼"
        LOGGER.info(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}, å‘¨æœŸ {self.epoch}: åœ¨ {mode_str} ä¸‹é‡æ–°è®¡ç®—ç´¢å¼•å®Œæˆã€‚"
                    f"ä½¿ç”¨äº† {self._effective_total_sub_videos} ä¸ªå­è§†é¢‘)ã€‚"
                    f"ä¸ºæ­¤å‰¯æœ¬ç”Ÿæˆçš„æ€»å¸§ç´¢å¼•æ•°: {len(self.flat_frame_indices)}ã€‚")

    def _recalculate_indices(self) -> None:
        """
        é‡æ–°è®¡ç®—å½“å‰å‘¨æœŸçš„æ‰“ä¹±ã€åˆ†å¸ƒå¼åˆ†é…å’Œå±•å¹³åçš„å¸§ç´¢å¼•åˆ—è¡¨ã€‚
        ä½¿ç”¨ PyTorch å¼ é‡æ“ä½œé«˜æ•ˆå®Œæˆå±•å¹³ã€‚
        ç”± __init__ å’Œ set_epoch è°ƒç”¨ã€‚
        """
        if self._num_original_sub_videos == 0:
            self.flat_frame_indices = []
            self._num_sub_videos_per_replica = 0
            self._effective_total_sub_videos = 0
            LOGGER.warning(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: æ•°æ®é›†æ²¡æœ‰å­è§†é¢‘ã€‚é‡‡æ ·å™¨å°†ä¸ä¼šç”Ÿæˆä»»ä½•ç´¢å¼•ã€‚")
            return

        # 1. ç”Ÿæˆå¹¶æ‰“ä¹±åŸå§‹å­è§†é¢‘ç´¢å¼• [0, ..., _num_original_sub_videos - 1]
        sub_video_indices = list(range(self._num_original_sub_videos))
        if self.shuffle and self.is_training:
            g = torch.Generator()
            # åŸºäºå‘¨æœŸå’Œéšæœºç§å­ç”Ÿæˆï¼Œç¡®ä¿ä¸åŒå‘¨æœŸæœ‰ä¸åŒæ‰“ä¹±é¡ºåºï¼Œä¸”åŒä¸€å‘¨æœŸåœ¨ä¸åŒå‰¯æœ¬ä¸Šæ‰“ä¹±é¡ºåºç›¸åŒ
            g.manual_seed(self.seed + self.epoch)
            sub_video_indices = torch.randperm(self._num_original_sub_videos, generator=g).tolist()

        # 2. ç¡®å®šæ‰€æœ‰å‰¯æœ¬å°†ä½¿ç”¨çš„å­è§†é¢‘æ€»æ•°
        # è¿™æ˜¯è®­ç»ƒæ¨¡å¼ä¸‹åº”ç”¨ä¸¢å¼ƒ/ä¿®å‰ªé€»è¾‘çš„åœ°æ–¹
        if self.is_training:
            # æ ¹æ®è¦æ±‚ï¼Œæ€»å­è§†é¢‘æ•°å¿…é¡»æ˜¯ (batch_size * num_replicas) çš„æ•´æ•°å€ã€‚
            # è¿™é‡Œçš„ num_replicas å·²ç»æ˜¯æ ¹æ® distribute å‚æ•°è°ƒæ•´åçš„å€¼ (åˆ†å¸ƒå¼ >=1, å•å¡=1)
            trim_denominator = self.batch_size * self.num_replicas
            if trim_denominator == 0: # é¿å…é™¤ä»¥é›¶ï¼Œå°½ç®¡ batch_size å’Œ num_replicas åº”è¯¥éƒ½æ˜¯æ­£æ•°
                total_sub_videos_to_use = 0
            else:
                total_sub_videos_to_use = (self._num_original_sub_videos // trim_denominator) * trim_denominator

            if total_sub_videos_to_use == 0 and self._num_original_sub_videos > 0:
                LOGGER.warning(
                    f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œbatch_size={self.batch_size} å’Œ num_replicas={self.num_replicas} (æœ‰æ•ˆå€¼)ï¼Œ"
                    f"ä¿®å‰ªåˆ†æ¯ä¸º {trim_denominator}ã€‚åŸå§‹å­è§†é¢‘æ€»æ•° ({self._num_original_sub_videos}) å°‘äºæ­¤åˆ†æ¯ã€‚"
                    f"æ‰€æœ‰å­è§†é¢‘éƒ½å°†è¢«èˆå¼ƒï¼Œé‡‡æ ·å™¨å°†ä¸ä¼šç”Ÿæˆä»»ä½•ç´¢å¼•ã€‚"
                )

        else: # æµ‹è¯•/éªŒè¯æ¨¡å¼
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„å­è§†é¢‘ï¼Œä¸è¿›è¡ŒåŸºäº batch_size çš„ä¿®å‰ª
            total_sub_videos_to_use = self._num_original_sub_videos

            # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œå½“ distribute=False æ—¶ num_replicas=1ï¼Œ
            # total_sub_videos_to_use æ€»æ˜¯èƒ½è¢« num_replicas æ•´é™¤ï¼Œå› æ­¤ä¸ä¼šè§¦å‘å­è§†é¢‘åˆ—è¡¨å¡«å……ã€‚
            # å¦‚æœ distribute=True (åˆ†å¸ƒå¼æµ‹è¯•)ï¼Œä¼šè¿›è¡Œå­è§†é¢‘åˆ—è¡¨å¡«å……ä»¥ç¡®ä¿å‡åŒ€åˆ†é…ã€‚
            if self.num_replicas > 0 and total_sub_videos_to_use % self.num_replicas != 0:
                padding_size = self.num_replicas - (total_sub_videos_to_use % self.num_replicas)
                # ç”¨åˆ—è¡¨å¼€å¤´çš„éƒ¨åˆ†è¿›è¡Œå¡«å……
                sub_video_indices_to_pad = sub_video_indices[:total_sub_videos_to_use]
                sub_video_indices = sub_video_indices_to_pad + sub_video_indices_to_pad[:padding_size]
                total_sub_videos_to_use = len(sub_video_indices)
                assert total_sub_videos_to_use % self.num_replicas == 0 # å¡«å……ååº”èƒ½æ•´é™¤


        # ä¿®å‰ª/é€‰æ‹©æ‰“ä¹±åçš„å­è§†é¢‘ç´¢å¼•åˆ—è¡¨åˆ°ç¡®å®šçš„æ€»æ•°
        sub_video_indices = sub_video_indices[:total_sub_videos_to_use]
        self._effective_total_sub_videos = len(sub_video_indices) # ç»è¿‡ä¿®å‰ªæˆ–å¡«å……åçš„å­è§†é¢‘æ€»æ•°


        # 3. åœ¨å‰¯æœ¬ä¹‹é—´åˆ†é…å·²é€‰å®šçš„å­è§†é¢‘ç´¢å¼•
        # åœ¨å•å¡æ¨¡å¼ä¸‹ (num_replicas=1, rank=0)ï¼Œstart_idx_replica=0, end_idx_replica=_effective_total_sub_videos,
        # replica_sub_video_indices å°†åŒ…å«æ‰€æœ‰ _effective_total_sub_videos ä¸ªå­è§†é¢‘çš„ç´¢å¼•ã€‚
        # åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œæ­£å¸¸åˆ†é…ã€‚
        assert self.num_replicas > 0, "num_replicas å¿…é¡»å¤§äº 0"
        assert self._effective_total_sub_videos % self.num_replicas == 0, \
            f"é”™è¯¯: ç»è¿‡è°ƒæ•´çš„æ€»å­è§†é¢‘æ•° ({self._effective_total_sub_videos}) å¿…é¡»èƒ½è¢«è¿›ç¨‹æ•° ({self.num_replicas}) æ•´é™¤ã€‚"

        self._num_sub_videos_per_replica = self._effective_total_sub_videos // self.num_replicas

        start_idx_replica = self.rank * self._num_sub_videos_per_replica
        end_idx_replica = start_idx_replica + self._num_sub_videos_per_replica
        replica_sub_video_indices = sub_video_indices[start_idx_replica:end_idx_replica]

        assert len(replica_sub_video_indices) == self._num_sub_videos_per_replica, \
            f"è¿›ç¨‹ {self.rank+1}: å‰¯æœ¬å­è§†é¢‘æ•°é‡ä¸åŒ¹é…ã€‚é¢„æœŸ {self._num_sub_videos_per_replica}ï¼Œå®é™… {len(replica_sub_video_indices)}"

        # 4. ä¸ºå½“å‰å‰¯æœ¬é«˜æ•ˆå±•å¹³å¸§ç´¢å¼•åˆ—è¡¨
        # ä½¿ç”¨ PyTorch å¼ é‡æ“ä½œï¼Œå³ä½¿å­è§†é¢‘é•¿åº¦ä¸åŒ (æµ‹è¯•æ¨¡å¼) æˆ–ç›¸åŒ (è®­ç»ƒæ¨¡å¼)ã€‚
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œå­è§†é¢‘é•¿åº¦ä¸åŒï¼Œéœ€è¦å¡«å……ä»¥ä¾¿æ„å»ºå¼ é‡ã€‚
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œå­è§†é¢‘é•¿åº¦ç›¸åŒï¼Œå¡«å……è™½ç„¶ä»£ç ä¸Šå­˜åœ¨ï¼Œä½†å®é™…ä¸ä¼šå¡«å……ã€‚
        self.flat_frame_indices = []
        if not replica_sub_video_indices:
            LOGGER.warning(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: åˆ†å‘/ä¿®å‰ªåæ²¡æœ‰å­è§†é¢‘åˆ†é…åˆ°æ­¤å‰¯æœ¬ã€‚")
            return

        # è·å–åˆ†é…åˆ°çš„å­è§†é¢‘çš„å®é™…å¸§ç´¢å¼•åˆ—è¡¨
        replica_frame_lists = [self.dataset.sub_videos[sv_idx]
                            for sv_idx in replica_sub_video_indices
                            # å†æ¬¡æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
                            if 0 <= sv_idx < len(self.dataset.sub_videos)]

        if not replica_frame_lists:
            self.flat_frame_indices = []
            return

        # æ‰¾åˆ°è¿™äº›å­è§†é¢‘ä¸­çš„æœ€å¤§é•¿åº¦
        max_len = max(len(lst) for lst in replica_frame_lists)

        if max_len == 0:
            self.flat_frame_indices = []
            LOGGER.warning(f"è¿›ç¨‹ {self.rank+1}/{self.num_replicas}: åˆ†é…åˆ°çš„å­è§†é¢‘åˆ—è¡¨çš„æœ€å¤§é•¿åº¦ä¸º 0.")
            return

        # åˆ›å»ºä¸€ä¸ªå¡«å……å€¼ï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªåœ¨ dataset.im_files ç´¢å¼•èŒƒå›´ä¹‹å¤–çš„å€¼
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œå­è§†é¢‘é•¿åº¦ä¸åŒï¼Œéœ€è¦å¡«å……å¼ é‡ã€‚è®­ç»ƒæ¨¡å¼ä¸‹ä¸éœ€è¦å®é™…å¡«å……ã€‚
        # å¡«å……å€¼ (-1) åœ¨æœ€ç»ˆçš„ self.flat_frame_indices åˆ—è¡¨ä¸­ä¼šè¢«è¿‡æ»¤æ‰ã€‚
        padding_value = -1 # ä½¿ç”¨ -1 ä½œä¸ºå¡«å……å€¼

        # åˆ›å»ºä¸€ä¸ª PyTorch å¼ é‡æ¥å­˜å‚¨å¡«å……åçš„å¸§ç´¢å¼•
        # å½¢çŠ¶ä¸º [åˆ†é…ç»™æ­¤å‰¯æœ¬çš„å­è§†é¢‘æ•°, è¿™äº›è§†é¢‘ä¸­çš„æœ€å¤§é•¿åº¦]
        padded_tensor = torch.full((len(replica_frame_lists), max_len), padding_value, dtype=torch.long)

        # å°†æ¯ä¸ªå­è§†é¢‘çš„å¸§ç´¢å¼•å¤åˆ¶åˆ°å¼ é‡ä¸­
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œlen(frame_list) == max_lenï¼Œå¡«å……ä¸ä¼šå®é™…å‘ç”Ÿã€‚
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œlen(frame_list) <= max_lenï¼Œä¼šå‘ç”Ÿå¡«å……ã€‚
        for i, frame_list in enumerate(replica_frame_lists):
            if frame_list: # é¿å…å¤„ç†ç©ºåˆ—è¡¨å¯¼è‡´åˆ‡ç‰‡é”™è¯¯
                # ç¡®ä¿ frame_list ä¸­çš„ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œå°½ç®¡é€šå¸¸æ˜¯è¿™æ ·
                if all(0 <= idx < len(self.dataset.im_files) for idx in frame_list):
                    padded_tensor[i, :len(frame_list)] = torch.tensor(frame_list, dtype=torch.long)
                else:
                    LOGGER.error(f"è¿›ç¨‹ {self.rank+1}: å­è§†é¢‘ {replica_sub_video_indices[i]} (åŸå§‹ç´¢å¼•) åŒ…å«è¶…å‡ºæ•°æ®é›†æ€»å¸§æ•°èŒƒå›´çš„å¸§ç´¢å¼•.")
                    # å¯ä»¥é€‰æ‹©åœ¨æ­¤å¤„è·³è¿‡æ­¤å­è§†é¢‘æˆ–æ¸…ç©º flat_frame_indices
                    # ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œåªè®°å½•é”™è¯¯ï¼Œå¹¶è®©å¡«å……å€¼ä¸º -1 ä¿æŒï¼Œåç»­ä¼šè¢«è¿‡æ»¤
                    pass # ä¿æŒå¡«å……å€¼ä¸º-1

        # è½¬ç½®å¼ é‡ï¼Œä½¿å…¶å½¢çŠ¶å˜ä¸º [æœ€å¤§é•¿åº¦, åˆ†é…ç»™æ­¤å‰¯æœ¬çš„å­è§†é¢‘æ•°]
        # è¿™ä½¿å¾—æŒ‰æ—¶é—´æ­¥éå†æ›´åŠ è‡ªç„¶ (éå†ç¬¬ä¸€ç»´)
        N, length = padded_tensor.shape
        padded_tensor = padded_tensor.reshape(N//self.batch_size, self.batch_size, length)
        transposed_tensor = padded_tensor.permute(0, 2, 1).reshape(-1, self.batch_size)

        # å±•å¹³å¼ é‡ï¼ŒåŒæ—¶è¿‡æ»¤æ‰å¡«å……å€¼ (-1)ã€‚
        # æ— è®ºè®­ç»ƒè¿˜æ˜¯æµ‹è¯•æ¨¡å¼ï¼Œå¡«å……å€¼éƒ½ä¸ä¼šå‡ºç°åœ¨æœ€ç»ˆè¾“å‡ºåˆ—è¡¨ä¸­ã€‚
        self.flat_frame_indices = transposed_tensor[transposed_tensor != padding_value].tolist()

        # å†æ¬¡éªŒè¯ç”Ÿæˆçš„ç´¢å¼•èŒƒå›´ (å¯é€‰ä½†æ¨è)
        if self.flat_frame_indices:
            min_idx = min(self.flat_frame_indices)
            max_idx = max(self.flat_frame_indices)
            if min_idx < 0 or max_idx >= len(self.dataset.im_files):
                LOGGER.error(f"è¿›ç¨‹ {self.rank+1}: ç”Ÿæˆçš„å¸§ç´¢å¼•è¶…å‡ºæ•°æ®é›†æ€»å¸§æ•°èŒƒå›´ã€‚min={min_idx}, max={max_idx}, total_frames={len(self.dataset.im_files)}.")


        



    def __iter__(self) -> Iterator[int]:
        """ç”Ÿæˆå½“å‰å‘¨æœŸé¢„å…ˆè®¡ç®—å¥½çš„å¸§ç´¢å¼•ã€‚"""
        # åœ¨åˆå§‹åŒ–æ—¶ç«‹å³è®¡ç®—å¹¶å­˜å‚¨ç´¢å¼• (ç”¨äº Epoch 0)
        # ç›´æ¥è¿”å›é¢„å…ˆè®¡ç®—å¥½çš„åˆ—è¡¨çš„è¿­ä»£å™¨
        # LOGGER.info(f"Rank {self.rank}/{self.num_replicas}, Epoch {self.epoch}: Starting iteration.")
        return iter(self.flat_frame_indices)

    def __len__(self) -> int:
        """è¿”å›æ­¤å‰¯æœ¬åœ¨æ­¤å‘¨æœŸå°†ç”Ÿæˆçš„æ€»å¸§ç´¢å¼•æ•°é‡ã€‚"""
        # é•¿åº¦å°±æ˜¯é¢„å…ˆè®¡ç®—å¥½çš„åˆ—è¡¨çš„é•¿åº¦
        return len(self.flat_frame_indices)

    def set_epoch(self, epoch: int) -> None:
        """
        è®¾ç½®æ­¤é‡‡æ ·å™¨çš„å‘¨æœŸç¼–å·å¹¶é‡æ–°è®¡ç®—ç´¢å¼•ã€‚
        å½“ `shuffle=True` æ—¶ï¼Œè¿™ç¡®ä¿æ‰€æœ‰å‰¯æœ¬åœ¨æ¯ä¸ªå‘¨æœŸä½¿ç”¨ä¸åŒçš„éšæœºé¡ºåº
        è¿›è¡Œå­è§†é¢‘æ‰“ä¹±ï¼Œå¹¶ä¸”ç´¢å¼•ä¼šæ ¹æ®è¿™ä¸ªæ–°çš„æ‰“ä¹±é‡æ–°ç”Ÿæˆã€‚
        """
        # çˆ¶ç±»çš„ set_epoch ä»…æ›´æ–° self.epoch å’Œ self.seedã€‚
        # æˆ‘ä»¬éœ€è¦ç¡®ä¿åœ¨ epoch å˜åŒ–æ—¶é‡æ–°è®¡ç®—ç´¢å¼•ã€‚
        super().set_epoch(epoch) # æ›´æ–° self.epoch å’Œå†…éƒ¨éšæœºç§å­çŠ¶æ€
        # LOGGER.info(f"Rank {self.rank}/{self.num_replicas}: Setting epoch to {epoch} and recalculating indices.")
        self._recalculate_indices() # é‡æ–°è®¡ç®—ç´¢å¼•ä»¥åæ˜ æ–°çš„å‘¨æœŸæ‰“ä¹±
            
from torch.utils.data.sampler import BatchSampler
class InfiniteDataLoader(dataloader.DataLoader):
    """
    Dataloader that reuses workers.

    Uses same syntax as vanilla DataLoader.
    """

    def __init__(self, *args, **kwargs):
        """Dataloader that infinitely recycles workers, inherits from DataLoader."""
        super().__init__(*args, **kwargs)
        object.__setattr__(self, "batch_sampler", _RepeatSampler(self.batch_sampler))
        self.iterator = super().__iter__()

    def __len__(self):
        """Returns the length of the batch sampler's sampler."""
        return len(self.batch_sampler.sampler)

    def __iter__(self):
        """Creates a sampler that repeats indefinitely."""
        for _ in range(len(self)):
            yield next(self.iterator)

    def reset(self):
        """
        Reset iterator.

        This is useful when we want to modify settings of dataset while training.
        """
        self.iterator = self._get_iterator()

class _RepeatSampler:
    """
    Sampler that repeats forever.

    Args:
        sampler (Dataset.sampler): The sampler to repeat.
    """

    def __init__(self, sampler):
        """Initializes an object that repeats a given sampler indefinitely."""
        self.sampler = sampler

    def __iter__(self):
        """Iterates over the 'sampler' and yields its contents."""
        while True:
            yield from iter(self.sampler)


def seed_worker(worker_id):  # noqa
    """Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader."""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def build_yolo_dataset(cfg, img_path, batch, data, mode="train", rect=False, stride=32, multi_modal=False,images_dir=None,
                 labels_dir=None,):
    """Build YOLO Dataset."""
    # dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    datasetname = data.get('datasetname', 'YOLODataset')
    if datasetname == "YOLODataset":
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    elif datasetname in ("YOLOStreamDataset",  "YOLOVideoDataset"):
        dataset = YOLOStreamDataset
    # elif datasetname == "YOLOVideoDataset":
    #     dataset = YOLOVideoDataset
    else:
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
        
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == "train",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == "train" else 0.5,
        prefix=colorstr(f"{mode}: "),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == "train" else 1.0,
        images_dir = images_dir,
        labels_dir = labels_dir
    )
    

def build_yoloft_dataset(cfg, img_path, batch, data, mode="train", rect=False, stride=32, multi_modal=False,images_dir=None,
                 labels_dir=None,):
    """Build YOLO Dataset."""
    # dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    datasetname = data.get('datasetname', 'YOLODataset')
    if datasetname == "YOLODataset":
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    elif datasetname in ("YOLOStreamDataset"):
        dataset = YOLOStreamDataset
    elif datasetname == "YOLOVideoDataset":
        dataset = YOLOVideoDataset
    else:
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
        
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == "train",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == "train" else 0.5,
        prefix=colorstr(f"{mode}: "),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == "train" else 1.0,
        images_dir = images_dir,
        labels_dir = labels_dir
    )
    
def build_yoloft_val_dataset(cfg, img_path, batch, data, mode="train", rect=False, stride=32, multi_modal=False,images_dir=None,
                 labels_dir=None,):
    """Build YOLO Dataset."""
    # dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    datasetname = data.get('datasetname', 'YOLODataset')
    if datasetname == "YOLODataset":
        dataset = YOLOMultiModalDataset if multi_modal else YOLODataset
    else:
        dataset = YOLOStreamDataset
        LOGGER.info("YOLOFT Network val and test use YOLOStreamDataset")
        
    return dataset(
        img_path=img_path,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == "train",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == "train" else 0.5,
        prefix=colorstr(f"{mode}: "),
        task=cfg.task,
        classes=cfg.classes,
        data=data,
        fraction=cfg.fraction if mode == "train" else 1.0,
        images_dir = images_dir,
        labels_dir = labels_dir
    )
    
def build_grounding(cfg, img_path, json_file, batch, mode="train", rect=False, stride=32):
    """Build YOLO Dataset."""
    return GroundingDataset(
        img_path=img_path,
        json_file=json_file,
        imgsz=cfg.imgsz,
        batch_size=batch,
        augment=mode == "train",  # augmentation
        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function
        rect=cfg.rect or rect,  # rectangular batches
        cache=cfg.cache or None,
        single_cls=cfg.single_cls or False,
        stride=int(stride),
        pad=0.0 if mode == "train" else 0.5,
        prefix=colorstr(f"{mode}: "),
        task=cfg.task,
        classes=cfg.classes,
        fraction=cfg.fraction if mode == "train" else 1.0,
    )


def build_dataloader(dataset, batch, workers, shuffle=True, rank=-1):
    """Return an InfiniteDataLoader or DataLoader for training or validation set."""
    batch = min(batch, len(dataset))
    nd = torch.cuda.device_count()  # number of CUDA devices
    nw = min(os.cpu_count() // max(nd, 1), workers)  # number of workers
    sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)
    generator = torch.Generator()
    generator.manual_seed(6148914691236517205 + RANK)
    return InfiniteDataLoader(
        dataset=dataset,
        batch_size=batch,
        shuffle=shuffle and sampler is None,
        num_workers=nw,
        sampler=sampler,
        pin_memory=PIN_MEMORY,
        collate_fn=getattr(dataset, "collate_fn", None),
        worker_init_fn=seed_worker,
        generator=generator,
    )


def build_stream_dataloader(dataset, batch, workers, shuffle=True, rank=-1):
    """Return an InfiniteDataLoader or DataLoader for training or validation set."""
    batch = min(batch, len(dataset))
    nd = torch.cuda.device_count()  # number of CUDA devices
    nw = min(os.cpu_count() // max(nd, 1), workers)  # number of workers
    sampler = StreamDistributedSampler(dataset, batch, seed=1, distributed=False) if rank == -1 else StreamDistributedSampler(dataset, batch, seed=1, distributed=True, shuffle=shuffle)
    generator = torch.Generator()
    generator.manual_seed(6148914691236517205 + RANK)
    return InfiniteDataLoader(
        dataset=dataset,
        batch_size=batch,
        shuffle=shuffle and sampler is None,
        num_workers=nw,
        sampler=sampler,
        pin_memory=PIN_MEMORY,
        collate_fn=getattr(dataset, "collate_fn", None),
        worker_init_fn=seed_worker,
        generator=generator,
    )

def build_video_dataloader(dataset, batch, workers, shuffle=True, rank=-1):
    """Return an InfiniteDataLoader or DataLoader for training or validation set."""
    batch = min(batch, len(dataset))
    nd = torch.cuda.device_count()  # number of CUDA devices
    nw = min(os.cpu_count() // max(nd, 1), workers)  # number of workers
    sampler = StreamDistributedSampler(dataset, batch, seed=1, distributed=False) if rank == -1 else StreamDistributedSampler(dataset, batch, seed=1, distributed=True, shuffle=shuffle)
    generator = torch.Generator()
    generator.manual_seed(6148914691236517205 + RANK)
    return InfiniteDataLoader(
        dataset=dataset,
        batch_size=batch,
        shuffle=shuffle and sampler is None,
        num_workers=nw,
        sampler=sampler,
        pin_memory=PIN_MEMORY,
        collate_fn=getattr(dataset, "collate_fn", None),
        worker_init_fn=seed_worker,
        generator=generator,
    )

# def build_stream_dataloader(dataset, batch, workers, shuffle=True, rank=-1):
#     """Return an InfiniteDataLoader or DataLoader for training or validation set."""
#     batch = min(batch, len(dataset))
#     nd = torch.cuda.device_count()  # number of CUDA devices
#     nw = min(os.cpu_count() // max(nd, 1), workers)  # number of workers
#     sampler = StreamSampler(dataset, batch, seed=1, distributed=False) if rank == -1 else StreamSampler(dataset, batch, seed=1, distributed=True)
#     generator = torch.Generator()
#     generator.manual_seed(6148914691236517205 + RANK)
#     return InfiniteDataLoader(
#         dataset=dataset,
#         batch_size=batch,
#         shuffle=shuffle and sampler is None,
#         num_workers=nw,
#         sampler=sampler,
#         pin_memory=PIN_MEMORY,
#         collate_fn=getattr(dataset, "collate_fn", None),
#         worker_init_fn=seed_worker,
#         generator=generator,
#     )
    
def check_source(source):
    """Check source type and return corresponding flag values."""
    webcam, screenshot, from_img, in_memory, tensor, buffer = False, False, False, False, False, False
    if isinstance(source, (str, int, Path)):  # int for local usb camera
        source = str(source)
        is_file = Path(source).suffix[1:] in (IMG_FORMATS | VID_FORMATS)
        is_url = source.lower().startswith(("https://", "http://", "rtsp://", "rtmp://", "tcp://"))
        webcam = source.isnumeric() or source.endswith(".streams") or (is_url and not is_file)
        screenshot = source.lower() == "screen"
        if is_url and is_file:
            source = check_file(source)  # download
    elif isinstance(source, LOADERS):
        in_memory = True
    elif isinstance(source, (list, tuple)):
        if len(source)==2 and len(source[-1]) == 3:
            if isinstance(source[0], torch.Tensor):
                tensor = True
            else:
                from_img = True
            buffer = True
        else:
            source = autocast_list(source)  # convert all list elements to PIL or np arrays
            from_img = True
    elif isinstance(source, (Image.Image, np.ndarray)):
        from_img = True
    elif isinstance(source, torch.Tensor):
        tensor = True
    else:
        raise TypeError("Unsupported image type. For supported types see https://docs.ultralytics.com/modes/predict")

    return source, webcam, screenshot, from_img, in_memory, tensor, buffer


def load_inference_source(source=None, batch=1, vid_stride=1, buffer=False):
    """
    Loads an inference source for object detection and applies necessary transformations.

    Args:
        source (str, Path, Tensor, PIL.Image, np.ndarray): The input source for inference.
        batch (int, optional): Batch size for dataloaders. Default is 1.
        vid_stride (int, optional): The frame interval for video sources. Default is 1.
        buffer (bool, optional): Determined whether stream frames will be buffered. Default is False.

    Returns:
        dataset (Dataset): A dataset object for the specified input source.
    """
    source, stream, screenshot, from_img, in_memory, tensor, buffer = check_source(source)
    source_type = source.source_type if in_memory else SourceTypes(stream, screenshot, from_img, tensor, buffer)

    # Dataloader
    if tensor and buffer:
        dataset = LoadTensorBuffer(source[0], source[1])
    elif tensor:
        dataset = LoadTensor(source)
    elif in_memory:
        dataset = source
    elif stream:
        dataset = LoadStreams(source, vid_stride=vid_stride, buffer=buffer)
    elif screenshot:
        dataset = LoadScreenshots(source)
    elif from_img and buffer:
        dataset = LoadPilAndNumpy(source[0], source[1])
        # AttributeError("error, img buffer have not achieve")
    elif from_img:
        dataset = LoadPilAndNumpy(source)
    else:
        dataset = LoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride)

    # Attach source types to the dataset
    setattr(dataset, "source_type", source_type)

    return dataset
