# Ultralytics YOLO ğŸš€, AGPL-3.0 license

from pathlib import Path
import os,json
from ultralytics import SAM, YOLO, YOLOFT
from collections import defaultdict
import torch
import cv2
from tqdm import tqdm

import os
import re
from pathlib import Path

def get_current_run_dir(output_dir="output_dir"):
    """
    è‡ªåŠ¨æ£€æµ‹output_dirä¸‹çš„run*æ–‡ä»¶å¤¹ï¼Œåˆ›å»ºä¸‹ä¸€ä¸ªç¼–å·çš„runæ–‡ä»¶å¤¹ï¼Œå¹¶è¿”å›è·¯å¾„ã€‚
    
    å‚æ•°:
        output_dir (str): çˆ¶ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º"output_dir"
    
    è¿”å›:
        str: æ–°åˆ›å»ºçš„runæ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼ˆå¦‚"output_dir/run4"ï¼‰
    """
    # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # æ­£åˆ™åŒ¹é…æ‰€æœ‰run{number}æ–‡ä»¶å¤¹å¹¶æå–æ•°å­—
    pattern = re.compile(r'^run(\d+)$')  # åŒ¹é…runå¼€å¤´+çº¯æ•°å­—çš„æ–‡ä»¶å¤¹å
    run_numbers = [
        int(match.group(1)) 
        for d in output_path.iterdir() 
        if d.is_dir() and (match := pattern.match(d.name))
    ]
    
    # è®¡ç®—ä¸‹ä¸€ä¸ªç¼–å·ï¼ˆå¦‚æœæ²¡æœ‰åŒ¹é…é¡¹åˆ™ä»0å¼€å§‹ï¼‰
    next_num = max(run_numbers) + 1 if run_numbers else 0
    
    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹
    current_run_dir = output_path / f"run{next_num}"
    current_run_dir.mkdir(exist_ok=False)  # å¦‚æœå·²å­˜åœ¨ä¼šæŠ¥é”™ï¼Œé¿å…è¦†ç›–
    
    return str(current_run_dir)

def xyxy2xywh(xyxy):
    """
    å°†è¾¹ç•Œæ¡†ä» [x1, y1, x2, y2] æ ¼å¼è½¬æ¢ä¸º [x, y, w, h] æ ¼å¼
    
    å‚æ•°:
        xyxy (list/tuple/numpy.ndarray): è¾“å…¥è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [x1, y1, x2, y2]
    
    è¿”å›:
        list: è½¬æ¢åçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [x, y, w, h]
    """
    x1, y1, x2, y2 = xyxy  # è§£æ„åæ ‡
    x = x1                  # å·¦ä¸Šè§’xåæ ‡
    y = y1                  # å·¦ä¸Šè§’yåæ ‡
    w = x2 - x1             # å®½åº¦
    h = y2 - y1             # é«˜åº¦
    return [x, y, w, h]

def results_compose(batch):
    """
    å°† list[dict] è½¬æ¢ä¸º dict[list]ï¼Œä¸ä¾èµ–å­—å…¸é”®çš„é¡ºåºã€‚
    
    Args:
        batch: List[Dict], è¾“å…¥å­—å…¸åˆ—è¡¨ï¼Œæ‰€æœ‰å­—å…¸å¿…é¡»æœ‰ç›¸åŒçš„é”®ï¼ˆé¡ºåºæ— å…³ï¼‰ã€‚
    
    Returns:
        Dict[List], é”®æ¥è‡ªè¾“å…¥å­—å…¸ï¼Œå€¼ä¸ºå¯¹åº”ä½ç½®çš„å€¼åˆ—è¡¨ã€‚
    """
    if not batch:
        return {}
    
    # è·å–ç¬¬ä¸€ä¸ªå­—å…¸çš„æ‰€æœ‰é”®ï¼ˆé¡ºåºæ— å…³ï¼‰
    keys = set(batch[0].keys())
    
    # éªŒè¯æ‰€æœ‰å­—å…¸çš„é”®æ˜¯å¦ä¸€è‡´
    for d in batch[1:]:
        if set(d.keys()) != keys:
            raise ValueError("è¾“å…¥å­—å…¸çš„é”®ä¸ä¸€è‡´ï¼")
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    new_batch = {k: [] for k in keys}
    
    # å¡«å……å€¼åˆ—è¡¨
    for d in batch:
        for k in keys:
            new_batch[k].append(d[k])
    
    return new_batch

import torch

def xywh2xyxy(bboxes: torch.Tensor) -> torch.Tensor:
    """
    å°† [n, 4] çš„ xywh æ ¼å¼è¾¹ç•Œæ¡†è½¬æ¢ä¸º xyxy æ ¼å¼
    Args:
        bboxes: torch.Tensor, shape [n, 4], æ ¼å¼ä¸º [x_top, y_left, width, height]
    Returns:
        torch.Tensor, shape [n, 4], æ ¼å¼ä¸º [x1, y1, x2, y2]
    """
    if not isinstance(bboxes, torch.Tensor):
        bboxes = torch.tensor(bboxes)
    
    # ç¡®ä¿è¾“å…¥æ˜¯äºŒç»´å¼ é‡ [n, 4]
    if bboxes.dim() == 1:
        bboxes = bboxes.unsqueeze(0)
    
    # è½¬æ¢é€»è¾‘
    x1 = bboxes[..., 0]  # x1 = x_top
    y1 = bboxes[..., 1]  # y1 = y_left
    x2 = bboxes[..., 0] + bboxes[..., 2]  # x2 = x_top + width
    y2 = bboxes[..., 1] + bboxes[..., 3]  # y2 = y_left + height
    
    return torch.stack([x1, y1, x2, y2], dim=-1)

def track_annotate(
    images_dir,
    ann_file,
    detect_model="hyper-yoloS_best.pt",
    tracker_yaml="bytetrack.yaml",
    device="",
    output_dir=None,
    show = False
):
    """
    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.

    This function processes images in a specified directory, detects objects using a YOLO model, and then generates
    segmentation masks using a SAM model. The resulting annotations are saved as text files.

    Args:
        data (str): Path to a folder containing images to be annotated.
        det_model (str): Path or name of the pre-trained YOLO detection model.
        sam_model (str): Path or name of the pre-trained SAM segmentation model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model; default is 0.25.
        iou (float): IoU threshold for filtering overlapping boxes in detection results; default is 0.45.
        imgsz (int): Input image resize dimension; default is 640.
        max_det (int): Limits detections per image to control outputs in dense scenes.
        classes (list): Filters predictions to specified class IDs, returning only relevant detections.
        output_dir (str | None): Directory to save the annotated results. If None, a default directory is created.

    Examples:
        >>> from ultralytics.data.annotator import auto_annotate
        >>> auto_annotate(data="ultralytics/assets", det_model="yolo11n.pt", sam_model="mobile_sam.pt")

    Notes:
        - The function creates a new directory for output if not specified.
        - Annotation results are saved as text files with the same names as the input images.
        - Each line in the output text file represents a detected object with its class ID and segmentation points.
    """
    model = YOLOFT(detect_model)
    if ann_file:
        with open(ann_file, 'rb') as f:
            anno_data = json.load(f)
    current_run_dir = get_current_run_dir(output_dir)
    os.makedirs(current_run_dir, exist_ok=True)
    print(f"tracking images_dir:{images_dir}, and results will save in {current_run_dir}")

    if ann_file:
        name_to_id = {vid["id"]: vid["name"] for vid in anno_data["videos"]}

        video_images = defaultdict(list)
        for img in anno_data["images"]:
            video_images[img["video_id"]].append(img)
        # å¯¹æ¯ä¸ª video_id çš„å›¾åƒåˆ—è¡¨æŒ‰ç…§ frame_id æ’åº
        for video_id in video_images:
            video_images[video_id].sort(key=lambda x: x["frame_id"])
    else:
        video_images = defaultdict(list)
        img_id = 0
        for video_name in os.listdir(images_dir):
            #åˆ¤æ–­æ˜¯ä¸æ˜¯dir
            if os.path.isdir(os.path.join(images_dir, video_name)):
                frame_file = os.listdir(os.path.join(images_dir, video_name))
                frame_file = sorted(frame_file, key=lambda x: int(x.split(".")[0]))
                for img_file in frame_file:
                    if img_file.endswith(('.jpg', '.png', '.jpeg')):
                        video_images[video_name].append({
                            "file_name": os.path.join(video_name, img_file),
                            "id": img_id,
                            "frame_id": int(img_file.split(".")[0])
                        })
                        img_id += 1

    track_json_results = []
    for video_id, frames in tqdm(video_images.items(),total=len(video_images), desc=f"tracking videos"):
        if ann_file:
            video_name = name_to_id[video_id]
        else:
            video_name = video_id

        if show:
            first_frame_path = os.path.join(images_dir, frames[0]["file_name"])
            # è·å–è§†é¢‘å®½é«˜ä¿¡æ¯
            orig_img = cv2.imread(first_frame_path)
            h, w, _ = orig_img.shape
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4ç¼–ç å™¨
            video_writer = cv2.VideoWriter(
                filename=os.path.join(current_run_dir, f"{video_name}.mp4"),
                fourcc=fourcc,
                fps=25,
                frameSize=(w,h),
            )

        buffer = [None, None, None]
        for frame_info in tqdm(frames, total=len(frames), desc=f"{video_name}: "):
            frame_path = os.path.join(images_dir, frame_info["file_name"])
            orig_img = cv2.imread(frame_path)
            assert orig_img is not None, f"Failed to load image {frame_path}"

            result = model.track((orig_img,buffer), tracker=tracker_yaml, persist=True)[0]

            if result.boxes and result.boxes.id is not None:
                boxes = result.boxes.xyxy.cpu().tolist()
                cls = result.boxes.cls.cpu().tolist()
                conf = result.boxes.conf.cpu().tolist()
                track_ids = result.boxes.id.int().cpu().tolist()
                for i, box in enumerate(boxes):
                    track_json_results.append({
                        "image_id" : frame_info["id"],
                        "category_id" : cls[i],
                        "bbox" : xyxy2xywh(box),
                        "score" : conf[i],
                        "track_id": track_ids[i],
                        "video_id": video_id
                    })

                if show:
                    annotated_frame = result.plot()
                    video_writer.write(annotated_frame)

        if show:
            video_writer.release()
    
    if ann_file:
        with open(os.path.join(current_run_dir, "tracks.json"), "w") as f:
            json.dump(track_json_results, f)


track_annotate(images_dir = "/data/jiahaoguo/datasets/gaode_6/true_videos/", 
              detect_model="runs/save/train107_yoloftS_dcn_dy_s3_t_gaode5&6_ramdominfaraed_e16_53.1/weights/best.pt",
              ann_file=None, 
              output_dir = "runs/track/", show=True)