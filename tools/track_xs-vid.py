# Ultralytics YOLO üöÄ, AGPL-3.0 license

from pathlib import Path
import os,json
from ultralytics import SAM, YOLO
from collections import defaultdict
import torch
import cv2
from tqdm import tqdm

import os
import re
from pathlib import Path
from eval.tao import TaoEval
import logging
# Ëé∑ÂèñÊ†πÊó•ÂøóËÆ∞ÂΩïÂô®
root_logger = logging.getLogger()
# ‰øÆÊîπÊó•ÂøóÁ∫ßÂà´‰∏∫INFO
root_logger.setLevel(logging.INFO)


def get_current_run_dir(output_dir="output_dir"):
    """
    Ëá™Âä®Ê£ÄÊµãoutput_dir‰∏ãÁöÑrun*Êñá‰ª∂Â§πÔºåÂàõÂª∫‰∏ã‰∏Ä‰∏™ÁºñÂè∑ÁöÑrunÊñá‰ª∂Â§πÔºåÂπ∂ËøîÂõûË∑ØÂæÑ„ÄÇ
    
    ÂèÇÊï∞:
        output_dir (str): Áà∂ÁõÆÂΩïË∑ØÂæÑÔºåÈªòËÆ§‰∏∫"output_dir"
    
    ËøîÂõû:
        str: Êñ∞ÂàõÂª∫ÁöÑrunÊñá‰ª∂Â§πÁöÑÂÆåÊï¥Ë∑ØÂæÑÔºàÂ¶Ç"output_dir/run4"Ôºâ
    """
    # Á°Æ‰øùÁà∂ÁõÆÂΩïÂ≠òÂú®
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Ê≠£ÂàôÂåπÈÖçÊâÄÊúârun{number}Êñá‰ª∂Â§πÂπ∂ÊèêÂèñÊï∞Â≠ó
    pattern = re.compile(r'^run(\d+)$')  # ÂåπÈÖçrunÂºÄÂ§¥+Á∫ØÊï∞Â≠óÁöÑÊñá‰ª∂Â§πÂêç
    run_numbers = [
        int(match.group(1)) 
        for d in output_path.iterdir() 
        if d.is_dir() and (match := pattern.match(d.name))
    ]
    
    # ËÆ°ÁÆó‰∏ã‰∏Ä‰∏™ÁºñÂè∑ÔºàÂ¶ÇÊûúÊ≤°ÊúâÂåπÈÖçÈ°πÂàô‰ªé0ÂºÄÂßãÔºâ
    next_num = max(run_numbers) + 1 if run_numbers else 0
    
    # ÂàõÂª∫Êñ∞Êñá‰ª∂Â§π
    current_run_dir = output_path / f"run{next_num}"
    current_run_dir.mkdir(exist_ok=False)  # Â¶ÇÊûúÂ∑≤Â≠òÂú®‰ºöÊä•ÈîôÔºåÈÅøÂÖçË¶ÜÁõñ
    
    return str(current_run_dir)

def xyxy2xywh(xyxy):
    """
    Â∞ÜËæπÁïåÊ°Ü‰ªé [x1, y1, x2, y2] Ê†ºÂºèËΩ¨Êç¢‰∏∫ [x, y, w, h] Ê†ºÂºè
    
    ÂèÇÊï∞:
        xyxy (list/tuple/numpy.ndarray): ËæìÂÖ•ËæπÁïåÊ°ÜÔºåÊ†ºÂºè‰∏∫ [x1, y1, x2, y2]
    
    ËøîÂõû:
        list: ËΩ¨Êç¢ÂêéÁöÑËæπÁïåÊ°ÜÔºåÊ†ºÂºè‰∏∫ [x, y, w, h]
    """
    x1, y1, x2, y2 = xyxy  # Ëß£ÊûÑÂùêÊ†á
    x = x1                  # Â∑¶‰∏äËßíxÂùêÊ†á
    y = y1                  # Â∑¶‰∏äËßíyÂùêÊ†á
    w = x2 - x1             # ÂÆΩÂ∫¶
    h = y2 - y1             # È´òÂ∫¶
    return [x, y, w, h]

def results_compose(batch):
    """
    Â∞Ü list[dict] ËΩ¨Êç¢‰∏∫ dict[list]Ôºå‰∏ç‰æùËµñÂ≠óÂÖ∏ÈîÆÁöÑÈ°∫Â∫è„ÄÇ
    
    Args:
        batch: List[Dict], ËæìÂÖ•Â≠óÂÖ∏ÂàóË°®ÔºåÊâÄÊúâÂ≠óÂÖ∏ÂøÖÈ°ªÊúâÁõ∏ÂêåÁöÑÈîÆÔºàÈ°∫Â∫èÊó†ÂÖ≥Ôºâ„ÄÇ
    
    Returns:
        Dict[List], ÈîÆÊù•Ëá™ËæìÂÖ•Â≠óÂÖ∏ÔºåÂÄº‰∏∫ÂØπÂ∫î‰ΩçÁΩÆÁöÑÂÄºÂàóË°®„ÄÇ
    """
    if not batch:
        return {}
    
    # Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™Â≠óÂÖ∏ÁöÑÊâÄÊúâÈîÆÔºàÈ°∫Â∫èÊó†ÂÖ≥Ôºâ
    keys = set(batch[0].keys())
    
    # È™åËØÅÊâÄÊúâÂ≠óÂÖ∏ÁöÑÈîÆÊòØÂê¶‰∏ÄËá¥
    for d in batch[1:]:
        if set(d.keys()) != keys:
            raise ValueError("ËæìÂÖ•Â≠óÂÖ∏ÁöÑÈîÆ‰∏ç‰∏ÄËá¥ÔºÅ")
    
    # ÂàùÂßãÂåñÁªìÊûúÂ≠óÂÖ∏
    new_batch = {k: [] for k in keys}
    
    # Â°´ÂÖÖÂÄºÂàóË°®
    for d in batch:
        for k in keys:
            new_batch[k].append(d[k])
    
    return new_batch

import torch

def xywh2xyxy(bboxes: torch.Tensor) -> torch.Tensor:
    """
    Â∞Ü [n, 4] ÁöÑ xywh Ê†ºÂºèËæπÁïåÊ°ÜËΩ¨Êç¢‰∏∫ xyxy Ê†ºÂºè
    Args:
        bboxes: torch.Tensor, shape [n, 4], Ê†ºÂºè‰∏∫ [x_top, y_left, width, height]
    Returns:
        torch.Tensor, shape [n, 4], Ê†ºÂºè‰∏∫ [x1, y1, x2, y2]
    """
    if not isinstance(bboxes, torch.Tensor):
        bboxes = torch.tensor(bboxes)
    
    # Á°Æ‰øùËæìÂÖ•ÊòØ‰∫åÁª¥Âº†Èáè [n, 4]
    if bboxes.dim() == 1:
        bboxes = bboxes.unsqueeze(0)
    
    # ËΩ¨Êç¢ÈÄªËæë
    x1 = bboxes[..., 0]  # x1 = x_top
    y1 = bboxes[..., 1]  # y1 = y_left
    x2 = bboxes[..., 0] + bboxes[..., 2]  # x2 = x_top + width
    y2 = bboxes[..., 1] + bboxes[..., 3]  # y2 = y_left + height
    
    return torch.stack([x1, y1, x2, y2], dim=-1)

def map_track_ids(result_anns):
    video_track_mapping = {}
    new_result_anns = []
    new_track_id = 0
    for ann in result_anns:
        video_id = ann['video_id']
        track_id = ann['track_id']
        unique_id = (video_id, track_id)
        if unique_id not in video_track_mapping:
            # ‰∏∫ÊØè‰∏™ËßÜÈ¢ëÁöÑÊØè‰∏™ track_id ÂàÜÈÖç‰∏Ä‰∏™Êñ∞ÁöÑÂîØ‰∏Ä ID
            video_track_mapping[unique_id] = {
                'track_id': new_track_id,
                "category_id": ann['category_id'],
            }
            new_track_id += 1

        new_ann = ann.copy()
        new_ann['track_id'] = video_track_mapping[unique_id]["track_id"]
        new_ann['category_id'] = video_track_mapping[unique_id]["category_id"]
        new_result_anns.append(new_ann)
    return new_result_anns

def track_annotate(
    images_dir,
    ann_file,
    detect_model="hyper-yoloS_best.pt",
    tracker_yaml="bytetrack.yaml",
    device="",
    output_dir=None,
    show = False
):
    """
    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.

    This function processes images in a specified directory, detects objects using a YOLO model, and then generates
    segmentation masks using a SAM model. The resulting annotations are saved as text files.

    Args:
        data (str): Path to a folder containing images to be annotated.
        det_model (str): Path or name of the pre-trained YOLO detection model.
        sam_model (str): Path or name of the pre-trained SAM segmentation model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model; default is 0.25.
        iou (float): IoU threshold for filtering overlapping boxes in detection results; default is 0.45.
        imgsz (int): Input image resize dimension; default is 640.
        max_det (int): Limits detections per image to control outputs in dense scenes.
        classes (list): Filters predictions to specified class IDs, returning only relevant detections.
        output_dir (str | None): Directory to save the annotated results. If None, a default directory is created.

    Examples:
        >>> from ultralytics.data.annotator import auto_annotate
        >>> auto_annotate(data="ultralytics/assets", det_model="yolo11n.pt", sam_model="mobile_sam.pt")

    Notes:
        - The function creates a new directory for output if not specified.
        - Annotation results are saved as text files with the same names as the input images.
        - Each line in the output text file represents a detected object with its class ID and segmentation points.
    """
    model = YOLO(detect_model)
    with open(ann_file, 'rb') as f:
        anno_data = json.load(f)
    current_run_dir = get_current_run_dir(output_dir)
    os.makedirs(current_run_dir, exist_ok=True)
    print(f"tracking images_dir:{images_dir}, and results will save in {current_run_dir}")

    name_to_id = {vid["id"]: vid["name"] for vid in anno_data["videos"]}

    video_images = defaultdict(list)
    for img in anno_data["images"]:
        video_images[img["video_id"]].append(img)
    # ÂØπÊØè‰∏™ video_id ÁöÑÂõæÂÉèÂàóË°®ÊåâÁÖß frame_id ÊéíÂ∫è
    for video_id in video_images:
        video_images[video_id].sort(key=lambda x: x["frame_id"])

    track_json_results = []
    for video_id, frames in tqdm(video_images.items(),total=len(video_images), desc=f"tracking videos"):
        video_name = name_to_id[video_id]

        if show:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4ÁºñÁ†ÅÂô®
            video_writer = cv2.VideoWriter(
                filename=os.path.join(current_run_dir, f"{video_name}.mp4"),
                fourcc=fourcc,
                fps=25,
                frameSize=(1024,1024),
            )

        for frame_info in tqdm(frames, total=len(frames), desc=f"{video_name}: "):
            frame_path = os.path.join(images_dir, frame_info["file_name"])
            orig_img = cv2.imread(frame_path)
            assert orig_img is not None, f"Failed to load image {frame_path}"

            result = model.track(orig_img, tracker=tracker_yaml, persist=True)[0]

            if result.boxes and result.boxes.id is not None:
                boxes = result.boxes.xyxy.cpu().tolist()
                cls = result.boxes.cls.cpu().tolist()
                conf = result.boxes.conf.cpu().tolist()
                track_ids = result.boxes.id.int().cpu().tolist()
                for i, box in enumerate(boxes):
                    track_json_results.append({
                        "image_id" : frame_info["id"],
                        "category_id" : cls[i],
                        "bbox" : xyxy2xywh(box),
                        "score" : conf[i],
                        "track_id": track_ids[i],
                        "video_id": video_id
                    })

                if show:
                    annotated_frame = result.plot()
                    video_writer.write(annotated_frame)

        if show:
            video_writer.release()
    
    track_json_results = map_track_ids(track_json_results)
    result_path = os.path.join(current_run_dir, "tracks.json")
    with open(result_path, "w") as f:
        json.dump(track_json_results, f)
        
    return result_path

ann_file = "/data/jiahaoguo/dataset/XS-VIDv2/annotations/jsonv2/test_segment.json"
test_pt = [
            # "runs/xs-vid/train125-hyperL_12e_26.2/weights/best.pt",
           "runs/xs-vid/hyper-yoloS_best.pt",
        #    "runs/xs-vid/train125_yolov8l_12e_25.9/weights/best.pt",
        #    "runs/xs-vid/train124_yolov8s_12e_23.6/weights/best.pt"
           ]

results_json_path = []
for pt in test_pt:
    ps = track_annotate(images_dir = "/data/jiahaoguo/dataset/XS-VIDv2/images/", 
                detect_model=pt,
                tracker_yaml="bytetrack.yaml",
                ann_file=ann_file, 
                output_dir = "runs/track/", show=False)
    results_json_path.append((pt, ps))
    
    
for (pt, result_path) in results_json_path:
    # TAO uses logging to print results. Make sure logging is set to show INFO
    # messages, or you won't see any evaluation results.
    print(f"Evaluating detect_model is {pt}")
    tao_eval = TaoEval(ann_file,
                    result_path, 
                    )
    tao_eval.run()
    tao_eval.print_results()

