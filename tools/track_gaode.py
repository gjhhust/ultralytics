# Ultralytics YOLO ğŸš€, AGPL-3.0 license

from pathlib import Path
import os,json
from ultralytics import SAM, YOLO
from collections import defaultdict
import torch
import cv2
from tqdm import tqdm

import os
import re
from pathlib import Path
from eval.tao import TaoEval
import logging
# è·å–æ ¹æ—¥å¿—è®°å½•å™¨
root_logger = logging.getLogger()
# ä¿®æ”¹æ—¥å¿—çº§åˆ«ä¸ºINFO
root_logger.setLevel(logging.INFO)


def get_current_run_dir(output_dir="output_dir"):
    """
    è‡ªåŠ¨æ£€æµ‹output_dirä¸‹çš„run*æ–‡ä»¶å¤¹ï¼Œåˆ›å»ºä¸‹ä¸€ä¸ªç¼–å·çš„runæ–‡ä»¶å¤¹ï¼Œå¹¶è¿”å›è·¯å¾„ã€‚
    
    å‚æ•°:
        output_dir (str): çˆ¶ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º"output_dir"
    
    è¿”å›:
        str: æ–°åˆ›å»ºçš„runæ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼ˆå¦‚"output_dir/run4"ï¼‰
    """
    # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # æ­£åˆ™åŒ¹é…æ‰€æœ‰run{number}æ–‡ä»¶å¤¹å¹¶æå–æ•°å­—
    pattern = re.compile(r'^run(\d+)$')  # åŒ¹é…runå¼€å¤´+çº¯æ•°å­—çš„æ–‡ä»¶å¤¹å
    run_numbers = [
        int(match.group(1)) 
        for d in output_path.iterdir() 
        if d.is_dir() and (match := pattern.match(d.name))
    ]
    
    # è®¡ç®—ä¸‹ä¸€ä¸ªç¼–å·ï¼ˆå¦‚æœæ²¡æœ‰åŒ¹é…é¡¹åˆ™ä»0å¼€å§‹ï¼‰
    next_num = max(run_numbers) + 1 if run_numbers else 0
    
    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹
    current_run_dir = output_path / f"run{next_num}"
    current_run_dir.mkdir(exist_ok=False)  # å¦‚æœå·²å­˜åœ¨ä¼šæŠ¥é”™ï¼Œé¿å…è¦†ç›–
    
    return str(current_run_dir)

def xyxy2xywh(xyxy):
    """
    å°†è¾¹ç•Œæ¡†ä» [x1, y1, x2, y2] æ ¼å¼è½¬æ¢ä¸º [x, y, w, h] æ ¼å¼
    
    å‚æ•°:
        xyxy (list/tuple/numpy.ndarray): è¾“å…¥è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [x1, y1, x2, y2]
    
    è¿”å›:
        list: è½¬æ¢åçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [x, y, w, h]
    """
    x1, y1, x2, y2 = xyxy  # è§£æ„åæ ‡
    x = x1                  # å·¦ä¸Šè§’xåæ ‡
    y = y1                  # å·¦ä¸Šè§’yåæ ‡
    w = x2 - x1             # å®½åº¦
    h = y2 - y1             # é«˜åº¦
    return [x, y, w, h]

def results_compose(batch):
    """
    å°† list[dict] è½¬æ¢ä¸º dict[list]ï¼Œä¸ä¾èµ–å­—å…¸é”®çš„é¡ºåºã€‚
    
    Args:
        batch: List[Dict], è¾“å…¥å­—å…¸åˆ—è¡¨ï¼Œæ‰€æœ‰å­—å…¸å¿…é¡»æœ‰ç›¸åŒçš„é”®ï¼ˆé¡ºåºæ— å…³ï¼‰ã€‚
    
    Returns:
        Dict[List], é”®æ¥è‡ªè¾“å…¥å­—å…¸ï¼Œå€¼ä¸ºå¯¹åº”ä½ç½®çš„å€¼åˆ—è¡¨ã€‚
    """
    if not batch:
        return {}
    
    # è·å–ç¬¬ä¸€ä¸ªå­—å…¸çš„æ‰€æœ‰é”®ï¼ˆé¡ºåºæ— å…³ï¼‰
    keys = set(batch[0].keys())
    
    # éªŒè¯æ‰€æœ‰å­—å…¸çš„é”®æ˜¯å¦ä¸€è‡´
    for d in batch[1:]:
        if set(d.keys()) != keys:
            raise ValueError("è¾“å…¥å­—å…¸çš„é”®ä¸ä¸€è‡´ï¼")
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    new_batch = {k: [] for k in keys}
    
    # å¡«å……å€¼åˆ—è¡¨
    for d in batch:
        for k in keys:
            new_batch[k].append(d[k])
    
    return new_batch

import torch

def xywh2xyxy(bboxes: torch.Tensor) -> torch.Tensor:
    """
    å°† [n, 4] çš„ xywh æ ¼å¼è¾¹ç•Œæ¡†è½¬æ¢ä¸º xyxy æ ¼å¼
    Args:
        bboxes: torch.Tensor, shape [n, 4], æ ¼å¼ä¸º [x_top, y_left, width, height]
    Returns:
        torch.Tensor, shape [n, 4], æ ¼å¼ä¸º [x1, y1, x2, y2]
    """
    if not isinstance(bboxes, torch.Tensor):
        bboxes = torch.tensor(bboxes)
    
    # ç¡®ä¿è¾“å…¥æ˜¯äºŒç»´å¼ é‡ [n, 4]
    if bboxes.dim() == 1:
        bboxes = bboxes.unsqueeze(0)
    
    # è½¬æ¢é€»è¾‘
    x1 = bboxes[..., 0]  # x1 = x_top
    y1 = bboxes[..., 1]  # y1 = y_left
    x2 = bboxes[..., 0] + bboxes[..., 2]  # x2 = x_top + width
    y2 = bboxes[..., 1] + bboxes[..., 3]  # y2 = y_left + height
    
    return torch.stack([x1, y1, x2, y2], dim=-1)

def map_track_ids(result_anns):
    video_track_mapping = {}
    new_result_anns = []
    new_track_id = 0
    for ann in result_anns:
        video_id = ann['video_id']
        track_id = ann['track_id']
        unique_id = (video_id, track_id)
        if unique_id not in video_track_mapping:
            # ä¸ºæ¯ä¸ªè§†é¢‘çš„æ¯ä¸ª track_id åˆ†é…ä¸€ä¸ªæ–°çš„å”¯ä¸€ ID
            video_track_mapping[unique_id] = {
                'track_id': new_track_id,
                "category_id": ann['category_id'],
            }
            new_track_id += 1

        new_ann = ann.copy()
        new_ann['track_id'] = video_track_mapping[unique_id]["track_id"]
        new_ann['category_id'] = video_track_mapping[unique_id]["category_id"]
        new_result_anns.append(new_ann)
    return new_result_anns

def track_annotate(
    images_dir,
    ann_file,
    detect_model="hyper-yoloS_best.pt",
    tracker_yaml="bytetrack.yaml",
    device="",
    output_dir=None,
    show = False
):
    """
    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.

    This function processes images in a specified directory, detects objects using a YOLO model, and then generates
    segmentation masks using a SAM model. The resulting annotations are saved as text files.

    Args:
        data (str): Path to a folder containing images to be annotated.
        det_model (str): Path or name of the pre-trained YOLO detection model.
        sam_model (str): Path or name of the pre-trained SAM segmentation model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model; default is 0.25.
        iou (float): IoU threshold for filtering overlapping boxes in detection results; default is 0.45.
        imgsz (int): Input image resize dimension; default is 640.
        max_det (int): Limits detections per image to control outputs in dense scenes.
        classes (list): Filters predictions to specified class IDs, returning only relevant detections.
        output_dir (str | None): Directory to save the annotated results. If None, a default directory is created.

    Examples:
        >>> from ultralytics.data.annotator import auto_annotate
        >>> auto_annotate(data="ultralytics/assets", det_model="yolo11n.pt", sam_model="mobile_sam.pt")

    Notes:
        - The function creates a new directory for output if not specified.
        - Annotation results are saved as text files with the same names as the input images.
        - Each line in the output text file represents a detected object with its class ID and segmentation points.
    """
    model = YOLO(detect_model)
    with open(ann_file, 'rb') as f:
        anno_data = json.load(f)
    current_run_dir = get_current_run_dir(output_dir)
    os.makedirs(current_run_dir, exist_ok=True)
    print(f"tracking images_dir:{images_dir}, and results will save in {current_run_dir}")

    name_to_id = {vid["id"]: vid["name"] for vid in anno_data["videos"]}

    video_images = defaultdict(list)
    for img in anno_data["images"]:
        video_images[img["video_id"]].append(img)
    # å¯¹æ¯ä¸ª video_id çš„å›¾åƒåˆ—è¡¨æŒ‰ç…§ frame_id æ’åº
    for video_id in video_images:
        video_images[video_id].sort(key=lambda x: x["frame_id"])

    track_json_results = []
    for video_id, frames in tqdm(video_images.items(),total=len(video_images), desc=f"tracking videos"):
        video_name = name_to_id[video_id]

        if show:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4ç¼–ç å™¨
            video_writer = cv2.VideoWriter(
                filename=os.path.join(current_run_dir, f"{video_name}.mp4"),
                fourcc=fourcc,
                fps=25,
                frameSize=(1024,1024),
            )

        for frame_info in tqdm(frames, total=len(frames), desc=f"{video_name}: "):
            frame_path = os.path.join(images_dir, frame_info["file_name"])
            # orig_img = cv2.imread(frame_path)
            # assert orig_img is not None, f"Failed to load image {frame_path}"

            result = model.track(frame_path, tracker=tracker_yaml, persist=True)[0]

            if result.boxes and result.boxes.id is not None:
                boxes = result.boxes.xyxy.cpu().tolist()
                cls = result.boxes.cls.cpu().tolist()
                conf = result.boxes.conf.cpu().tolist()
                track_ids = result.boxes.id.int().cpu().tolist()
                for i, box in enumerate(boxes):
                    track_json_results.append({
                        "image_id" : frame_info["id"],
                        "category_id" : cls[i],
                        "bbox" : xyxy2xywh(box),
                        "score" : conf[i],
                        "track_id": track_ids[i],
                        "video_id": video_id
                    })

                if show:
                    annotated_frame = result.plot()
                    video_writer.write(annotated_frame)

        if show:
            video_writer.release()
    
    track_json_results = map_track_ids(track_json_results)
    result_path = os.path.join(current_run_dir, "tracks.json")
    with open(result_path, "w") as f:
        json.dump(track_json_results, f)
        
    return result_path

ann_file = "/data/jiahaoguo/ultralytics_yoloft/ultralytics/tools/gaode_time/tao_gt/task1_test.json"

result_path = track_annotate(images_dir = "/data/jiahaoguo/dataset/gaode_6/images/", 
                detect_model="runs/save/train217_yolos_newdata/weights/best.pt",
                tracker_yaml="bytetrack.yaml",
                ann_file=ann_file, 
                output_dir = "runs/track/", show=False)
    
    
    # TAO uses logging to print results. Make sure logging is set to show INFO
    # messages, or you won't see any evaluation results.
print(f"Evaluating detect_model is ")
tao_eval = TaoEval(ann_file,
                result_path, 
                )
tao_eval.run()
tao_eval.print_results()

